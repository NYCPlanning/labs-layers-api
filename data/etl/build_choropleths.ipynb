{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "To generate the csv files for the decennial datasets needed for this notebook, run this sql query against the 2020 and 2010 tables\n",
                "\n",
                "```\n",
                "select d.variable, d.value, d.geoid, g.geotype from decennial.\"2020\" d\n",
                "join support_geoids g\n",
                "on d.geoid = g.geoid\n",
                "where d.variable in ('popu18_1','popu18_1p','pop1','popperacre','wnh','bnh','anh','hsp1','wnhp','bnhp','anhp','hsp1p')\n",
                "and g.geotype like '%NTA%'\n",
                "```"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "import pandas as pd\n",
                "import geopandas as gp"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "# Import decennial datasets\n",
                "twenty = pd.read_csv('decennial_2020.csv')\n",
                "ten = pd.read_csv('decennial_2010.csv')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "# Pivot variables, indexing on geoid\n",
                "twenty_pivot = pd.pivot_table(twenty, values='value', columns='variable', index=['geoid']).reset_index()\n",
                "ten_pivot = pd.pivot_table(ten, values='value', columns='variable', index=['geoid']).reset_index()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "base_variables = ['popu18_1','popu18_1p','pop1','popperacre','wnh','bnh','anh','hsp1','wnhp','bnhp','anhp','hsp1p']\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "twenty_column_name_map = {}\n",
                "ten_column_name_map = {}\n",
                "for variable in base_variables:\n",
                "  twenty_column_name_map[variable] = variable+'_2020'\n",
                "  ten_column_name_map[variable] = variable+'_2010'\n",
                "\n",
                "twenty_column_name_map"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "{'popu18_1': 'popu18_1_2020',\n",
                            " 'popu18_1p': 'popu18_1p_2020',\n",
                            " 'pop1': 'pop1_2020',\n",
                            " 'popperacre': 'popperacre_2020',\n",
                            " 'wnh': 'wnh_2020',\n",
                            " 'bnh': 'bnh_2020',\n",
                            " 'anh': 'anh_2020',\n",
                            " 'hsp1': 'hsp1_2020',\n",
                            " 'wnhp': 'wnhp_2020',\n",
                            " 'bnhp': 'bnhp_2020',\n",
                            " 'anhp': 'anhp_2020',\n",
                            " 'hsp1p': 'hsp1p_2020'}"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 43
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "# Append year to variables so that 2010 and 2020 datasets can be combined\n",
                "ten_pivot.rename(columns=ten_column_name_map, inplace=True)\n",
                "twenty_pivot.rename(columns=twenty_column_name_map, inplace=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "data = ten_pivot.merge(twenty_pivot, on=\"geoid\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "count_variables = ['popu18_1','pop1','popperacre','wnh','bnh','anh','hsp1']"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "source": [
                "# Calculate change over time\n",
                "for variable in count_variables:\n",
                "  data[variable+\"_c\"] = data[variable+\"_2020\"] - data[variable+\"_2010\"]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "source": [
                "# Calculate percent change over time\n",
                "for variable in count_variables:\n",
                "  data[variable+\"_pc\"] = data[variable+\"_c\"] / data[variable+\"_2010\"] * 100"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "To generate the ACS datasets needed for this notebook, run this query\n",
                "\n",
                "```\n",
                "SELECT \n",
                " _popu181.geoid,\n",
                " support_geoids.\"label\",\n",
                " _popu181.popu181,\n",
                " _mdgr.mdgr,\n",
                " _pbwpv.pbwpv,\n",
                " _pbwpv.pbwpv_p,\n",
                " _lgoenlep1.lgoenlep1,\n",
                " _fb1.fb1_p,\n",
                " _ea_bchdh.ea_bchdh,\n",
                " _ea_bchdh.ea_bchdh_p,\n",
                " _pop65pl1.pop65pl1\n",
                "FROM (\n",
                " SELECT geoid, e as popu181\n",
                " FROM acs.\"2019\"\n",
                " WHERE geotype LIKE 'NTA%'\n",
                " AND variable = 'popu181'\n",
                ") _popu181\n",
                "LEFT JOIN (\n",
                " SELECT geoid, e as mdgr\n",
                " FROM acs.\"2019\"\n",
                " WHERE geotype LIKE 'NTA%'\n",
                " AND variable = 'mdgr'\n",
                ") _mdgr ON _popu181.geoid = _mdgr.geoid\n",
                "LEFT JOIN (\n",
                " SELECT geoid, e as pbwpv, p as pbwpv_p\n",
                " FROM acs.\"2019\"\n",
                " WHERE geotype LIKE 'NTA%'\n",
                " AND variable = 'pbwpv'\n",
                ") _pbwpv ON _popu181.geoid = _pbwpv.geoid\n",
                "LEFT JOIN (\n",
                " SELECT geoid, e as lgoenlep1\n",
                " FROM acs.\"2019\"\n",
                " WHERE geotype LIKE 'NTA%'\n",
                " AND variable = 'lgoenlep1'\n",
                ") _lgoenlep1 ON _popu181.geoid = _lgoenlep1.geoid\n",
                "LEFT JOIN (\n",
                " SELECT geoid, p as fb1_p\n",
                " FROM acs.\"2019\"\n",
                " WHERE geotype LIKE 'NTA%'\n",
                " AND variable = 'fb1'\n",
                ") _fb1 ON _popu181.geoid = _fb1.geoid\n",
                "LEFT JOIN (\n",
                " SELECT geoid, e as ea_bchdh, p as ea_bchdh_p\n",
                " FROM acs.\"2019\"\n",
                " WHERE geotype LIKE 'NTA%'\n",
                " AND variable = 'ea_bchdh'\n",
                ") _ea_bchdh ON _popu181.geoid = _ea_bchdh.geoid\n",
                "LEFT JOIN (\n",
                " SELECT geoid, e as pop65pl1\n",
                " FROM acs.\"2019\"\n",
                " WHERE geotype LIKE 'NTA%'\n",
                " AND variable = 'pop65pl1'\n",
                ") _pop65pl1 ON _popu181.geoid = _pop65pl1.geoid\n",
                "LEFT JOIN support_geoids\n",
                "ON _popu181.geoid = support_geoids.geoid\n",
                "WHERE support_geoids.geotype LIKE 'NTA%';\n",
                "```"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "source": [
                "# Import acs data and merge to decennial data (the sql above takes care of shaping this dataset for us)\n",
                "acs = pd.read_csv('acs.csv')\n",
                "data = data.merge(acs, on=\"geoid\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "# Drop 2010 columns and rename 2020 ones\n",
                "data.drop(ten_column_name_map.values(),axis=1, inplace=True)\n",
                "data.rename(columns={v: k for k, v in twenty_column_name_map.items()}, inplace=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "from shapely import wkt"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Run this query against the NTA geographies in PostGIS to generate a geopandas compatible dataset\n",
                "```\n",
                "select ST_AsText(wkb_geometry) as geometry, nta2020, ntaname from dcp_ntaboundaries\n",
                "```"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "source": [
                "# Import geographies and merge into geopandas dataframe\n",
                "_boundaries = pd.read_csv('nta_boundaries.csv')\n",
                "geometry = _boundaries['geometry'].map(wkt.loads)\n",
                "_boundaries = _boundaries.drop('geometry', axis=1).rename(columns={'nta2020': 'geoid'})\n",
                "ntas = gp.GeoDataFrame(_boundaries, crs=\"EPSG:4326\", geometry=geometry)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "ntas = ntas.merge(data, on='geoid')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "source": [
                "# Export fully formed geojson to json file. The contents of this file can be copie and pasted into the `data`\n",
                "# property of the json found in `/data/sources`\n",
                "ntas.to_file('ntas.json', driver=\"GeoJSON\")"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.10 64-bit ('3.7.10': pyenv)"
        },
        "interpreter": {
            "hash": "a5f03556caa2215ab11d9a0961a00401103cfc6c312c18edffc99ca8b891cf5b"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}