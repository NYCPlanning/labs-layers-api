{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to generate acs choropleths\n",
    "Use this script to update the choropleth data for ACS when new years are released. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the data source\n",
    "The choropleth layer is downstream from the database that serves population factfinder. In general, the upstream database is updated first with the new year's ACS data. Once the \n",
    "database is updated, a new choropleth can be generated from the data.\n",
    "\n",
    "The SQL queries listed within this notebook should be run against a local copy of the factfinder database,\n",
    "preferably running within a Docker container. To setup a local copy of the database, follow the steps listed\n",
    "in the `labs-factfinder-api` `README.md`. More specifically:\n",
    "\n",
    "- clone the `labs-factfinder-api`\n",
    "- start the docker instance of postgres `docker-compose up`\n",
    "- update the `.env` file to point to `docker`\n",
    "  -  `DATABASE_URL=postgresql://factfinder-user:factfinder-password@localhost:5432/factfinder-local`\n",
    "\n",
    "With the database running locally, connect to it with a postgres adminstrator tool, such as [postico](https://eggerapps.at/postico2/) or [pgAdmin](https://www.pgadmin.org/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the environment\n",
    "VSCode supports running Juypter notebooks within its editor. To configure a VSCode environment:\n",
    "\n",
    "- Open the `process-data` folder within its own VSCode workspace. This will help VSCode find the virtual environment created in the following steps.\n",
    "- Within the process-data folder, create and activate a virtual environment\n",
    "  - `python -m venv venv` (Some environments use the `python3` alias. Use the python alias for your system that points to python v3)\n",
    "  - `source venv/bin/activate`\n",
    "  - *Please use the `venv` naming convention, as the .gitignore specifically looks for it.*\n",
    "- Install the requirements in the activated virtualenv\n",
    "  - `pip install -r requirements.txt`\n",
    "- Open the `build_choropleth.ipynb` file\n",
    "- Run each code block as needed\n",
    "  - If prompted for a virtual environment, select `Python Environments` and then `venv`; agree to install any additional requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Choropleth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the csv files for the decennial datasets are not already available in the repository, run this sql query against the 2020 and 2010 decennial tables.  \n",
    "Replace \"2020\" with the approriate year.\n",
    "\n",
    "```\n",
    "select d.variable, d.value, d.geoid, g.geotype from decennial.\"2020\" d\n",
    "join support_geoids g\n",
    "on d.geoid = g.geoid\n",
    "where d.variable in ('popu18_1','popu18_1p','pop1','popperacre','wnh','bnh','anh','hsp1','wnhp','bnhp','anhp','hsp1p')\n",
    "and g.geotype like '%NTA%'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries. Note you will also need to have fiona installed as geopandas relies on it for writing to geojson\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import decennial datasets\n",
    "twenty = pd.read_csv('decennial_2020.csv')\n",
    "ten = pd.read_csv('decennial_2010.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot variables, indexing on geoid\n",
    "twenty_pivot = pd.pivot_table(twenty, values='value', columns='variable', index=['geoid']).reset_index()\n",
    "ten_pivot = pd.pivot_table(ten, values='value', columns='variable', index=['geoid']).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_variables = ['popu18_1','popu18_1p','pop1','popperacre','wnh','bnh','anh','hsp1','wnhp','bnhp','anhp','hsp1p']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'popu18_1': 'popu18_1_2020',\n",
       " 'popu18_1p': 'popu18_1p_2020',\n",
       " 'pop1': 'pop1_2020',\n",
       " 'popperacre': 'popperacre_2020',\n",
       " 'wnh': 'wnh_2020',\n",
       " 'bnh': 'bnh_2020',\n",
       " 'anh': 'anh_2020',\n",
       " 'hsp1': 'hsp1_2020',\n",
       " 'wnhp': 'wnhp_2020',\n",
       " 'bnhp': 'bnhp_2020',\n",
       " 'anhp': 'anhp_2020',\n",
       " 'hsp1p': 'hsp1p_2020'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_column_name_map = {}\n",
    "ten_column_name_map = {}\n",
    "for variable in base_variables:\n",
    "  twenty_column_name_map[variable] = variable+'_2020'\n",
    "  ten_column_name_map[variable] = variable+'_2010'\n",
    "\n",
    "twenty_column_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append year to variables so that 2010 and 2020 datasets can be combined\n",
    "ten_pivot.rename(columns=ten_column_name_map, inplace=True)\n",
    "twenty_pivot.rename(columns=twenty_column_name_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ten_pivot.merge(twenty_pivot, on=\"geoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_variables = ['popu18_1','pop1','popperacre','wnh','bnh','anh','hsp1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate change over time\n",
    "for variable in count_variables:\n",
    "  data[variable+\"_c\"] = data[variable+\"_2020\"] - data[variable+\"_2010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent change over time\n",
    "for variable in count_variables:\n",
    "  data[variable+\"_pc\"] = data[variable+\"_c\"] / data[variable+\"_2010\"] * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the ACS datasets needed for this notebook, run this query.\n",
    "Replace \"2020\" with the current year.\n",
    "\n",
    "```\n",
    "SELECT \n",
    " _popu181.geoid,\n",
    " support_geoids.\"label\",\n",
    " _popu181.popu181,\n",
    " _mdgr.mdgr,\n",
    " _pbwpv.pbwpv,\n",
    " _pbwpv.pbwpv_p,\n",
    " _lgoenlep1.lgoenlep1,\n",
    " _fb1.fb1_p,\n",
    " _ea_bchdh.ea_bchdh,\n",
    " _ea_bchdh.ea_bchdh_p,\n",
    " _pop65pl1.pop65pl1\n",
    "FROM (\n",
    " SELECT geoid, estimate as popu181\n",
    " FROM acs.\"2020\"\n",
    " WHERE geotype LIKE 'NTA%'\n",
    " AND variable = 'popu181'\n",
    ") _popu181\n",
    "LEFT JOIN (\n",
    " SELECT geoid, estimate as mdgr\n",
    " FROM acs.\"2020\"\n",
    " WHERE geotype LIKE 'NTA%'\n",
    " AND variable = 'mdgr'\n",
    ") _mdgr ON _popu181.geoid = _mdgr.geoid\n",
    "LEFT JOIN (\n",
    " SELECT geoid, estimate as pbwpv, percent as pbwpv_p\n",
    " FROM acs.\"2020\"\n",
    " WHERE geotype LIKE 'NTA%'\n",
    " AND variable = 'pbwpv'\n",
    ") _pbwpv ON _popu181.geoid = _pbwpv.geoid\n",
    "LEFT JOIN (\n",
    " SELECT geoid, estimate as lgoenlep1\n",
    " FROM acs.\"2020\"\n",
    " WHERE geotype LIKE 'NTA%'\n",
    " AND variable = 'lgoenlep1'\n",
    ") _lgoenlep1 ON _popu181.geoid = _lgoenlep1.geoid\n",
    "LEFT JOIN (\n",
    " SELECT geoid, percent as fb1_p\n",
    " FROM acs.\"2020\"\n",
    " WHERE geotype LIKE 'NTA%'\n",
    " AND variable = 'fb1'\n",
    ") _fb1 ON _popu181.geoid = _fb1.geoid\n",
    "LEFT JOIN (\n",
    " SELECT geoid, estimate as ea_bchdh, percent as ea_bchdh_p\n",
    " FROM acs.\"2020\"\n",
    " WHERE geotype LIKE 'NTA%'\n",
    " AND variable = 'ea_bchdh'\n",
    ") _ea_bchdh ON _popu181.geoid = _ea_bchdh.geoid\n",
    "LEFT JOIN (\n",
    " SELECT geoid, estimate as pop65pl1\n",
    " FROM acs.\"2020\"\n",
    " WHERE geotype LIKE 'NTA%'\n",
    " AND variable = 'pop65pl1'\n",
    ") _pop65pl1 ON _popu181.geoid = _pop65pl1.geoid\n",
    "LEFT JOIN support_geoids\n",
    "ON _popu181.geoid = support_geoids.geoid\n",
    "WHERE support_geoids.geotype LIKE 'NTA%';\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import acs data and merge to decennial data (the sql above takes care of shaping this dataset for us)\n",
    "acs = pd.read_csv('acs_2021.csv')\n",
    "data = data.merge(acs, on=\"geoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 2010 columns and rename 2020 ones\n",
    "data.drop(ten_column_name_map.values(),axis=1, inplace=True)\n",
    "data.rename(columns={v: k for k, v in twenty_column_name_map.items()}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To output GeoJSON, we need polygons of the NTA boundaries. The most recent boundaries as a csv should be available next to this notebook as `nta_boundaries.csv`. This csv was generated by running this query against the NTA geographies in PostGIS and exporting the query result as a csv. The PFF staging database has these polygons available in a table called `dcp_ntaboundaries` but in the future we should get these polygons from somewhere else, probably Carto.\n",
    "```\n",
    "select ST_AsText(wkb_geometry) as geometry, nta2020, ntaname, ntatype from dcp_ntaboundaries\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import geographies and merge into geopandas dataframe\n",
    "_boundaries = pd.read_csv('nta_boundaries.csv')\n",
    "geometry = _boundaries['geometry'].map(wkt.loads)\n",
    "_boundaries = _boundaries.drop('geometry', axis=1).rename(columns={'nta2020': 'geoid'})\n",
    "ntas = gp.GeoDataFrame(_boundaries, crs=\"EPSG:4326\", geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntas = ntas.merge(data, on='geoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non-residential NTAs, set all variable values to NaN\n",
    "columns_to_null = ['anh', 'anhp', 'bnh', 'bnhp',\n",
    "       'hsp1', 'hsp1p', 'pop1', 'popperacre', 'popu18_1', 'popu18_1p', 'wnh',\n",
    "       'wnhp', 'popu18_1_c', 'pop1_c', 'popperacre_c', 'wnh_c', 'bnh_c',\n",
    "       'anh_c', 'hsp1_c', 'popu18_1_pc', 'pop1_pc', 'popperacre_pc', 'wnh_pc',\n",
    "       'bnh_pc', 'anh_pc', 'hsp1_pc', 'popu181', 'mdgr', 'pbwpv',\n",
    "       'pbwpv_p', 'lgoenlep1', 'fb1_p', 'ea_bchdh', 'ea_bchdh_p', 'pop65pl1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ntas = ntas.copy()\n",
    "cleaned_ntas.loc[cleaned_ntas['ntatype']!=0,columns_to_null] = np.nan\n",
    "cleaned_ntas.drop('ntatype', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export fully formed geojson to json file. The contents of this file can be copied and pasted into the `data`\n",
    "# property of the json found in `/data/sources`\n",
    "cleaned_ntas.to_file('choropleths.json', driver=\"GeoJSON\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "394c62ab563a469ed0fc70e322f10c53ee4992e994668222c9e8a883c64e5e69"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('3.7.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
